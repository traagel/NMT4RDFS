{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def create_graph_words_lubm1():\n",
    "    \"\"\"\n",
    "    Reads the LUBM1 RDF data from ./data/jupyter/all_lubm.nt, extracts and processes the graph data using pandas.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame with two columns: 'input_graph_words' and 'inference_graph_words',\n",
    "      where each row contains a sequence of words representing a graph\n",
    "    \"\"\"\n",
    "\n",
    "    col_names=[\"subject\", \"predicate\", \"object\"]\n",
    "    data = pd.read_csv(\"./data/jupyter/all_lubm.nt\", delimiter=' ', names=col_names)\n",
    "    data['object'] = data['object'].apply(lambda x: x.rstrip('.'))\n",
    "\n",
    "    # Create a new column 'graph' by concatenating the subject, predicate, and object columns\n",
    "    data[\"graph\"] = data[\"subject\"] + \" \" + data[\"predicate\"] + \" \" + data[\"object\"]\n",
    "\n",
    "    # Group the data by 'graph' and aggregate the results into a single row\n",
    "    grouped_data = data.groupby(\"graph\").apply(\n",
    "    lambda x: pd.Series({\n",
    "        \"input_sequence\": x.iloc[:-1][\"object\"].str.cat(sep=\" \"),\n",
    "        \"target_sequence\": x.iloc[1:][\"object\"].str.cat(sep=\" \")\n",
    "    })).reset_index(level=1, drop=True).reset_index()\n",
    "\n",
    "    input_sequences = grouped_data[\"input_sequence\"]\n",
    "    inference_sequences = grouped_data[\"target_sequence\"]\n",
    "\n",
    "    df = pd.DataFrame({\"input_graph_words\": input_sequences, \"inference_graph_words\": inference_sequences})\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Too many levels: Index has only 1 level, not 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_5192\\3572018545.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_graph_words_lubm1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_5192\\414094790.py\u001B[0m in \u001B[0;36mcreate_graph_words_lubm1\u001B[1;34m()\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[1;34m\"input_sequence\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"object\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\" \"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[1;34m\"target_sequence\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"object\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\" \"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m     })).reset_index(level=1, drop=True).reset_index()\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[0minput_sequences\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgrouped_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"input_sequence\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\NMT4RDFS\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36mreset_index\u001B[1;34m(self, level, drop, inplace, col_level, col_fill)\u001B[0m\n\u001B[0;32m   4392\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtuple\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4393\u001B[0m                 \u001B[0mlevel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4394\u001B[1;33m             \u001B[0mlevel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_level_number\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlev\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mlev\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlevel\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4395\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m<\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnlevels\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4396\u001B[0m                 \u001B[0mnew_index\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdroplevel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\NMT4RDFS\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   4392\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtuple\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4393\u001B[0m                 \u001B[0mlevel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4394\u001B[1;33m             \u001B[0mlevel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_level_number\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlev\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mlev\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlevel\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4395\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m<\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnlevels\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4396\u001B[0m                 \u001B[0mnew_index\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdroplevel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\NMT4RDFS\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36m_get_level_number\u001B[1;34m(self, level)\u001B[0m\n\u001B[0;32m   1425\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1426\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_get_level_number\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1427\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_index_level\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1428\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1429\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\NMT4RDFS\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36m_validate_index_level\u001B[1;34m(self, level)\u001B[0m\n\u001B[0;32m   1419\u001B[0m                 raise IndexError(\"Too many levels:\"\n\u001B[0;32m   1420\u001B[0m                                  \u001B[1;34m\" Index has only 1 level, not %d\"\u001B[0m \u001B[1;33m%\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1421\u001B[1;33m                                  (level + 1))\n\u001B[0m\u001B[0;32m   1422\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mlevel\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1423\u001B[0m             raise KeyError('Level %s must be same as name (%s)' %\n",
      "\u001B[1;31mIndexError\u001B[0m: Too many levels: Index has only 1 level, not 2"
     ]
    }
   ],
   "source": [
    "df = create_graph_words_lubm1()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class GraphTranslationModel:\n",
    "    def __init__(self, df, latent_dim=256, batch_size=64, epochs=50):\n",
    "        self.input_seqs, self.target_seqs, self.input_chars, self.target_chars, self.num_encoder_tokens, \\\n",
    "        self.num_decoder_tokens, self.input_token_index, self.target_token_index, self.encoder_input_data, \\\n",
    "        self.decoder_input_data, self.decoder_target_data = self.prepare_data(df)\n",
    "\n",
    "        self.encoder, self.decoder, self.model = self.build_model(latent_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        input_seqs = df.apply(lambda row: row['subject'] + ' ' + row['predicate'], axis=1)\n",
    "        target_seqs = df['object']\n",
    "\n",
    "        # Create input and target character index mappings\n",
    "        input_chars = sorted(set(' '.join(input_seqs)))\n",
    "        target_chars = sorted(set(' '.join(target_seqs)))\n",
    "        num_encoder_tokens = len(input_chars)\n",
    "        num_decoder_tokens = len(target_chars)\n",
    "\n",
    "        input_token_index = dict([(char, i) for i, char in enumerate(input_chars)])\n",
    "        target_token_index = dict([(char, i) for i, char in enumerate(target_chars)])\n",
    "\n",
    "        # Encode input and target sequences as one-hot vectors\n",
    "        max_encoder_seq_length = max([len(txt) for txt in input_seqs])\n",
    "        max_decoder_seq_length = max([len(txt) for txt in target_seqs])\n",
    "\n",
    "        encoder_input_data = np.zeros((len(input_seqs), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "        decoder_input_data = np.zeros((len(input_seqs), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "        decoder_target_data = np.zeros((len(input_seqs), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "        for i, (input_seq, target_seq) in enumerate(zip(input_seqs, target_seqs)):\n",
    "            for t, char in enumerate(input_seq):\n",
    "                encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "            for t, char in enumerate(target_seq):\n",
    "                decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "                if t > 0:\n",
    "                    decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "        return input_seqs, target_seqs, input_chars, target_chars, num_encoder_tokens, num_decoder_tokens, \\\n",
    "               input_token_index, target_token_index, encoder_input_data, decoder_input_data, decoder_target_data\n",
    "\n",
    "    def build_model(self, input_dim=100, output_dim=50, hidden_units=256):\n",
    "        # Define the encoder layer\n",
    "        encoder_inputs = Input(shape=(None, input_dim))\n",
    "        encoder = LSTM(hidden_units, return_state=True)\n",
    "        encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "\n",
    "        # Define the decoder layer\n",
    "        decoder_inputs = Input(shape=(None, output_dim))\n",
    "        decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "        repeat_d_layer = RepeatVector(decoder_outputs.shape[1])\n",
    "        repeat_d = repeat_d_layer(encoder_outputs)\n",
    "        decoder_outputs = decoder_lstm(repeat_d, initial_state=encoder_states)[0]\n",
    "        decoder_dense = Dense(output_dim, activation='softmax')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        # Concatenate the encoder and decoder layers\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def translate(self, input_seq):\n",
    "        # Encode the input graph sequence as a one-hot vector\n",
    "        encoder_input_seq = np.zeros((1, len(input_seq), self.num_encoder_tokens), dtype='float32')\n",
    "        for t, char in enumerate(input_seq):\n",
    "            encoder_input_seq[0, t, self.input_token_index[char]] = 1.\n",
    "\n",
    "        # Get the initial encoder states\n",
    "        states_value = self.encoder.predict(encoder_input_seq)\n",
    "\n",
    "        # Generate the target text sequence using the decoder\n",
    "        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "        target_seq[0, 0, self.target_token_index['\\t']] = 1.\n",
    "\n",
    "        stop_condition = False\n",
    "        decoded_sentence = ''\n",
    "        while not stop_condition:\n",
    "            output_tokens, h, c = self.decoder.predict([target_seq] + states_value)\n",
    "\n",
    "            # Sample a token from the output distribution\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_char = self.target_chars[sampled_token_index]\n",
    "            decoded_sentence += sampled_char\n",
    "\n",
    "            # Exit condition: either hit max length or find stop character\n",
    "            if (sampled_char == '\\n' or len(decoded_sentence) > self.num_decoder_tokens):\n",
    "                stop_condition = True\n",
    "\n",
    "            # Update the target sequence with the sampled token\n",
    "            target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
    "            target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "            # Update the encoder states\n",
    "            states_value = [h, c]\n",
    "\n",
    "        return decoded_sentence\n",
    "\n",
    "    def train(self, val_split=0.2):\n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(self.encoder_input_data, self.decoder_input_data,\n",
    "                                                          test_size=val_split, random_state=42)\n",
    "\n",
    "        # Train the NMT model\n",
    "        history = self.model.fit([X_train, y_train], y_train, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                                 validation_data=([X_val, y_val], y_val))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
